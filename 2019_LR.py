# -*- coding: utf-8 -*-
"""Logistic Regresion_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ia40KtzXCEH7te8EHJbBHkeTB0keKRrF
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from statistics import mean
from sklearn.preprocessing import MinMaxScaler,StandardScaler
import timeit
# %matplotlib inline

class Log_class:
    #Assuming the arguments are numpy arrays(train,test) and learning rate alpha
    # extract features and concatenate bias term vector
    def __init__(self,x_train,x_test,y_train,y_test,alpha):
        self.xtrain=np.concatenate([x_train,np.ones((x_train.shape[0],1))],1) 
        self.xtest=np.concatenate([x_test,np.ones((x_test.shape[0],1))],axis=1) 
        self.ytrain=y_train
        self.ytest=y_test
        self.train_rate = alpha

    def sigmoid(self,z):#method to evaluate sigmoid function(used in fit method)
        sig = 1.0 / ( 1.0 + np.exp(- z))
        return sig
    
    def fit(self,err_threshold): #takes error threshold as arguments
        j=0                      #initialise iteration counter 
        self.error = []          #initialise empty list to store error 
        self.w = []              #initialise empty list to store weights
        m = self.xtrain.shape[1] #Number of features(including bias term)
        n = self.xtrain.shape[0] #Number of features(including bias term)
        self.w.append(np.zeros(shape=(m,))) #create w_0 as a vector of zeroes 
       
        a = self.sigmoid(z=(self.w[j].T @ self.xtrain.T))  #calculate sigma
        derivative =  self.xtrain.T @ (a-self.ytrain) #calculate gradient vector
       
        self.w.append(self.w[j]-self.train_rate*derivative) #calculate w_1 
        #calculate error for first iteration and add it to the list
        self.error.append(np.sum(np.square(self.w[j+1]-self.w[j]))) 
        
        #Repeat above procedure until error is below threshold
        while self.error[j]>err_threshold: 
            j+=1
            delta=0
            a = self.sigmoid(self.w[j].T @ self.xtrain.T)  
            derivative = self.xtrain.T @ (a-self.ytrain)
            self.w.append(self.w[j]-self.train_rate*derivative)             
            self.error.append(np.sum(np.square(self.w[j+1]-self.w[j])))
                              
            
            
        #plot graph of error vs iteration    
        plt.plot(range(1,len(self.error)+1),self.error,'ro') 
        plt.xlabel('Iteration')
        plt.ylabel('Error')
    
    #predicts ClassLabel using the final w_k vector from the fit function
    def predict(self,threshold=0.5):  #takes probability threshold as input
        sig_final = self.w[-1].T @ self.xtest.T
        self.probability = 1/(1+np.exp(-sig_final))
        self.predictions= (self.probability>threshold).astype(int)
        return self.predictions
    
    def accu_eval(self):#calculate accuracy,precision,recall and F1 score
        self.accuracy=(self.predictions==self.ytest).mean()
        #calculate true positives,false negatives,false negatives
        TP=np.logical_and(self.predictions==self.ytest , self.ytest==1).sum()  
        FN=np.logical_and(self.predictions==0 ,self.ytest==1).sum()           
        TN=np.logical_and(self.predictions==0, self.ytest==0).sum()           
        self.precision=TP/(self.predictions==1).sum()
        self.recall=TP/(TP+FN)
        self.f1_score=(2*self.precision*self.recall)/(self.precision
                                                      +self.recall)
        print('\t ACCURACY:',self.accuracy,'\n')
        print('\t PRECISION:',self.precision,'\n')
        print('\t RECALL:',self.recall,'\n')
        print('\t F1-SCORE:',self.f1_score,'\n')

class k_fold (Log_class): #inherits Log_class and all its methods
    def __init__(self,features,target,k,alpha,err_threshold,prob_threshold=0.5,
                 scaler=None): #overides __init__ inherited from Log_class
        #initialise counter and other parameters
        z=0
        #split the data into folds and store them in a list
        x_folds=np.array_split(features,k) 
        y_folds=np.array_split(target,k)
        self.train_rate = alpha       
        self.k_fold_accuracy=[]
        self.k_fold_precision=[]
        self.k_fold_recall=[]
        self.k_fold_f1_score=[]
        
            
        while z<k: #iterate k times
            print('ITERATION:',z+1,'\n')
            x_test= x_folds[z] #choose one fold for testing
            #choose other folds for training
            x_train = np.concatenate([x for i,x in enumerate(x_folds) if i!=z],
                                     axis=0) 
            y_test= y_folds[z] #choose one fold for testing
            #choose other folds for training
            y_train = np.concatenate([x for i,x in enumerate(y_folds) if i!=z],
                                     axis=0) 
           
            if scaler: # scale features
                x_train=scaler.fit_transform(x_train)
                x_test=scaler.transform(x_test)
            
            #split samples and target class labels for training,predictions and 
            #accuracy evaluation
            self.xtrain=np.concatenate([x_train,np.ones((x_train.shape[0],1))],
                                       axis=1) 
            self.xtest=np.concatenate([x_test,np.ones((x_test.shape[0],1))],
                                      axis=1) 
            self.ytrain=y_train
            self.ytest=y_test
            self.fit(err_threshold)#call fit functions from Log_class 
            self.predict(threshold=prob_threshold) #call predict function 
            self.accu_eval()       #call accu_eval function to evaluate metrics
            
            self.k_fold_accuracy.append(self.accuracy)   #store metrics 
            self.k_fold_precision.append(self.precision)
            self.k_fold_recall.append(self.recall)
            self.k_fold_f1_score.append(self.f1_score)
            
            z+=1                                          #iterate counter
            
        self.mean_acc = mean(self.k_fold_accuracy)   #calculate mean of metrics
        self.mean_precision = mean(self.k_fold_precision) 
        self.mean_recall = mean(self.k_fold_recall) 
        self.mean_f1_score = mean(self.k_fold_f1_score) 
       
        print('\nMEAN ACCURACY:',self.mean_acc,'\n')
        print('MEAN PRECISION:', self.mean_precision,'\n')
        print('MEAN RECALL:',self.mean_recall,'\n')
        print('MEAN F1-SCORE:',self.mean_f1_score,'\n')

"""**Dataset = Bankruptcy**"""

# Importing Datasets
# Dataset = Bankruptcy 
df1 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/bankrupcy.csv')

df_temp_1 = df1.copy()

train_1 = df_temp_1.sample(frac=0.8,random_state=100)
test_1 = df_temp_1.drop(train_1.index)

train_1 = train_1.to_numpy()
test_1 = test_1.to_numpy() 

xtrain_1 = train_1[:,:-1]
ytrain_1 = train_1[:,-1]
xtest_1 = test_1[:,:-1]
ytest_1 = test_1[:,-1]

#Adding Polynomial Features(square,cube,etc)(Optional)
poly_features =  dict({1: 5, 2: 12, 3:15, 4:26, 5:27, 6:29, 7:31, 8:36, 9:46, 
                       10:51, 11:53, 12:57, 13:59, 14:61}) 

for i in poly_features:
  col_select = poly_features[i]

  col_sq_train = np.power(xtrain_1[:,col_select],3) 
  col_sq_test = np.power(xtest_1[:,col_select],3)

  xtrain_1 = np.c_[xtrain_1,col_sq_train]
  xtest_1 =  np.c_[xtest_1,col_sq_test]

#SCALING FOR ALL FEATURES (Optional)
scaler=StandardScaler()
xtrain_1=scaler.fit_transform(xtrain_1)
xtest_1=scaler.transform(xtest_1)

#instance the model{alpha=learning rate}
bank = Log_class(xtrain_1, xtest_1, ytrain_1, ytest_1, alpha=0.005)

t1 = timeit.default_timer()   #Fit the model and note runtime
bank.fit(err_threshold=1e-5)
t2 = timeit.default_timer()
print('Run Time: ', (t2-t1),'seconds' )

bank.predict(threshold=0.5)
bank.accu_eval()

#code to run k fold cross validation on entire dataset
features_1 = np.r_[xtrain_1,xtest_1]
target_1 = np.r_[ytrain_1,ytest_1]

n_1= features_1.shape[0]
np.random.seed(0)
permutation = list(np.random.permutation(n_1))
features_1 = features_1[permutation,:]
target_1 = target_1[permutation]
scaler = StandardScaler()
t1 = timeit.default_timer()
kf_bank = k_fold(features = features_1, target = target_1, k=10, alpha=0.005, 
                 err_threshold=1e-5, prob_threshold=0.5,scaler=scaler)
t2 = timeit.default_timer()
print('Run Time: ', (t2-t1),'seconds' )

"""**Dataset = Hepatitis**"""

# Importing Datasets
# Dataset = Hepatitis


df2 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/hepatitis.csv')

df_temp_2 = df2.copy()

train_2 = df_temp_2.sample(frac=0.8,random_state=100)
test_2 = df_temp_2.drop(train_2.index)
train_2 = train_2.to_numpy()
test_2 = test_2.to_numpy()

# Splitting the feature data from Class labels
xtrain_2 = train_2[:,:-1]
ytrain_2 = train_2[:,-1]
xtest_2 = test_2[:,:-1]
ytest_2 = test_2[:,-1]

#Adding Polynomial Features(square,cube etc)
poly_features_2 =  dict({1: 14, 2:13, 3:17, 4:0, 5:16, 6:13 }) 

for i in poly_features_2:
  col_select = poly_features_2[i]

  col_sq_train = np.power(xtrain_2[:,col_select],2)
  col_sq_test = np.power(xtest_2[:,col_select],2)

  xtrain_2 = np.c_[xtrain_2,col_sq_train]
  xtest_2 =  np.c_[xtest_2,col_sq_test]

# NORMALIZE ONLY CONTINUOUS VARIABLES in HEPATITIS DATASET
#(RUn if needed, dont run if running kfold cross validation):
continuous_features=['age','alk_phosphate','sgot','protime','albumin',
                     'bilirubin']
cont_index=[]
for feature in continuous_features:
  cont_index.append(df_temp_2.columns.get_loc(feature))
for index in cont_index:
  train_2[:,index]=scaler.fit_transform(train_2[:,index].reshape(-1,1)).flatten(    
  )
  test_2[:,index]=scaler.transform(test_2[:,index].reshape(-1,1)).flatten()

#SCALING FOR ALL FEATURES (Optional)
scaler=StandardScaler()
xtrain_2=scaler.fit_transform(xtrain_2)
xtest_2=scaler.transform(xtest_2)

hep = Log_class(xtrain_2,xtest_2,ytrain_2,ytest_2,0.005)

t3 = timeit.default_timer()
hep.fit(err_threshold=1e-4)
t4 = timeit.default_timer()
print('Run Time: ', (t4-t3),'seconds' )

hep.predict(threshold=0.5)
hep.accu_eval()

#code to run kfold cross validation on entire dataset
features_2 = np.r_[xtrain_2,xtest_2]
target_2 = np.r_[ytrain_2,ytest_2]

n_2 = features_2.shape[0]

np.random.seed(1)
permutation_2 = list(np.random.permutation(n_2))
features_2 = features_2[permutation_2,:]
target_2 = target_2[permutation_2]
scaler=StandardScaler()
t3 = timeit.default_timer()
kf_hep = k_fold(features = features_2, target = target_2, k=10, alpha=0.005, 
                err_threshold=1e-4, prob_threshold=0.5,scaler=scaler)
t4 = timeit.default_timer()
print('Run Time: ', (t4-t3),'seconds' )

